import os
import math
import numpy as np
from osgeo import gdal
from copy import deepcopy
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn import functional as F
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, jaccard_score
import matplotlib.pyplot as plt
import time
import seaborn as sns
import pandas as pd

# Set device for GPU acceleration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# Defining function to read raster file and return array and datasource
def readraster(file):
    dataSource = gdal.Open(file)
    if dataSource is None:
        raise FileNotFoundError(f"Unable to open raster: {file}")
    band = dataSource.GetRasterBand(1)
    band = band.ReadAsArray()
    return dataSource, band


def identicalList(inList):
    inList = np.array(inList)
    logical = inList == inList[0]
    return bool(np.all(logical))


def builtupAreaDifference(landcover1, landcover2, buclass=1, cellsize=30):
    return np.sum(((landcover2 == buclass).astype(int) - (landcover1 == buclass).astype(int)) != 0) * (
            cellsize ** 2) / 1000000


# Defining class to read land cover file of multiple time periods
class LandCoverData:
    def __init__(self, file1, file2, file3=None):
        self.ds_lc1, self.arr_lc1 = readraster(file1)
        self.ds_lc2, self.arr_lc2 = readraster(file2)
        if file3:
            self.ds_lc3, self.arr_lc3 = readraster(file3)
        else:
            self.ds_lc3, self.arr_lc3 = None, None
        self.performChecks()

    def performChecks(self):
        # Check the rows and columns of input land cover datasets
        print("Checking the size of input rasters...")
        if (self.ds_lc1.RasterXSize == self.ds_lc2.RasterXSize) and (
                self.ds_lc1.RasterYSize == self.ds_lc2.RasterYSize):
            print("Land cover data size matched.")
            self.row, self.col = (self.ds_lc1.RasterYSize, self.ds_lc1.RasterXSize)
        else:
            raise ValueError("Input land cover files have different height and width.")
        # Check the number of classes in input land cover images
        print("\nChecking feature classes in land cover data...")
        unique1 = np.unique(self.arr_lc1)
        unique2 = np.unique(self.arr_lc2)
        print(f"Classes in first file: {unique1}")
        print(f"Classes in second file: {unique2}")

        if set(unique1) == set(unique2):
            print("The classes in input land cover files are matched.")
        else:
            print("Warning: Input land cover data have different class values.")
            # We'll proceed but note the difference


class GrowthFactors:
    def __init__(self, *args):
        self.gf = {}
        self.gf_ds = {}
        self.nFactors = len(args)
        n = 1
        for file in args:
            self.gf_ds[n], self.gf[n] = readraster(file)
            n += 1
        self.performChecks()

    def performChecks(self):
        print("\nChecking the size of input growth factors...")
        rows = []
        cols = []
        for n in range(1, self.nFactors + 1):
            rows.append(self.gf_ds[n].RasterYSize)
            cols.append(self.gf_ds[n].RasterXSize)
        if identicalList(rows) and identicalList(cols):
            print("Input factors have same row and column value.")
            self.row = rows[0]
            self.col = cols[0]
        else:
            raise ValueError("Input factors have different row and column value.")


# PyTorch Dataset class
class UrbanExpansionDataset(Dataset):
    def __init__(self, X, y):
        # X: (num_samples, channels, H, W)
        # y: (num_samples, 1, H, W)
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ConvLSTM Implementation with fixed hidden state initialization
class ConvLSTM(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(ConvLSTM, self).__init__()
        self.out_channels = out_channels
        # Convolution for input
        self.conv_x = nn.Conv2d(in_channels, 4 * out_channels, kernel_size, padding=kernel_size // 2)
        # Convolution for hidden state
        self.conv_h = nn.Conv2d(out_channels, 4 * out_channels, kernel_size, padding=kernel_size // 2)

    def init_hidden(self, batch_size, height, width, device):
        # Initialize hidden state and cell state with zeros
        return (torch.zeros(batch_size, self.out_channels, height, width, device=device),
                torch.zeros(batch_size, self.out_channels, height, width, device=device))

    def forward(self, x, hidden_state=None):
        # x shape: (batch, channels, H, W)
        batch_size, _, height, width = x.size()
        # Initialize hidden state if not provided
        if hidden_state is None:
            h, c = self.init_hidden(batch_size, height, width, x.device)
        else:
            h, c = hidden_state
        # Convolve input and hidden state
        x_conv = self.conv_x(x)
        h_conv = self.conv_h(h)
        combined = x_conv + h_conv
        i, f, g, o = torch.split(combined, self.out_channels, dim=1)
        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        g = torch.tanh(g)
        o = torch.sigmoid(o)
        c = f * c + i * g
        h = o * torch.tanh(c)
        return h


# ResConvLSTM Unit
class ResConvLSTMUnit(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(ResConvLSTMUnit, self).__init__()
        self.convlstm = ConvLSTM(in_channels, out_channels, kernel_size)
        self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()

    def forward(self, x):
        identity = x
        x = self.convlstm(x)
        if self.residual is not None:
            identity = self.residual(identity)
        x = x + identity
        x = self.bn(x)
        x = self.relu(x)
        return x


# Residual Block (standard conv residual)
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.shortcut is not None:
            identity = self.shortcut(identity)
        out = out + identity
        out = self.relu(out)
        return out


# PyTorch Model Definition (without ECPA)
class ResConvLSTMModel(nn.Module):
    def __init__(self, input_channels, patch_size=64):
        super(ResConvLSTMModel, self).__init__()
        self.patch_size = patch_size
        # Layer 1: First ResConvLSTM with 128 filters (three units)
        self.resconvlstm1 = ResConvLSTMUnit(input_channels, 128)
        self.resconvlstm2 = ResConvLSTMUnit(128, 128)
        self.resconvlstm3 = ResConvLSTMUnit(128, 128)
        # Layer 2: Second ResConvLSTM with 64 filters (three units)
        self.resconvlstm4 = ResConvLSTMUnit(128, 64)
        self.resconvlstm5 = ResConvLSTMUnit(64, 64)
        self.resconvlstm6 = ResConvLSTMUnit(64, 64)
        # Layer 3: Residual
        self.residual1 = ResidualBlock(64, 64)
        # Layer 4: ResConvLSTM with 32 filters
        self.resconvlstm7 = ResConvLSTMUnit(64, 32)
        # Layer 5: Residual connection
        self.residual2 = ResidualBlock(32, 32)
        # Layer 6: ResConvLSTM with 1 filter
        self.resconvlstm8 = ResConvLSTMUnit(32, 1)
        # Layer 7: Dense layer (1x1 convolution)
        self.final_conv = nn.Conv2d(1, 1, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x shape: (batch, channels, H, W)
        x = self.resconvlstm1(x)
        x = self.resconvlstm2(x)
        x = self.resconvlstm3(x)
        x = self.resconvlstm4(x)
        x = self.resconvlstm5(x)
        x = self.resconvlstm6(x)
        x = self.residual1(x)
        x = self.resconvlstm7(x)
        x = self.residual2(x)
        x = self.resconvlstm8(x)
        x = self.final_conv(x)
        x = self.sigmoid(x)
        return x

    def get_feature_maps(self, x):
        """Extract intermediate feature maps for analysis"""
        feature_maps = {}

        x1 = self.resconvlstm1(x)
        feature_maps['resconvlstm1'] = x1.detach().cpu().numpy()

        x2 = self.resconvlstm2(x1)
        feature_maps['resconvlstm2'] = x2.detach().cpu().numpy()

        x3 = self.resconvlstm3(x2)
        feature_maps['resconvlstm3'] = x3.detach().cpu().numpy()

        x4 = self.resconvlstm4(x3)
        feature_maps['resconvlstm4'] = x4.detach().cpu().numpy()

        x5 = self.resconvlstm5(x4)
        feature_maps['resconvlstm5'] = x5.detach().cpu().numpy()

        x6 = self.resconvlstm6(x5)
        feature_maps['resconvlstm6'] = x6.detach().cpu().numpy()

        x7 = self.residual1(x6)
        feature_maps['residual1'] = x7.detach().cpu().numpy()

        x8 = self.resconvlstm7(x7)
        feature_maps['resconvlstm7'] = x8.detach().cpu().numpy()

        x9 = self.residual2(x8)
        feature_maps['residual2'] = x9.detach().cpu().numpy()

        x10 = self.resconvlstm8(x9)
        feature_maps['resconvlstm8'] = x10.detach().cpu().numpy()

        return feature_maps


# Deep Learning CA Model
class DeepLearningCA:
    def __init__(self, landcover_data, growth_factors, patch_size=64):
        self.landcovers = landcover_data
        self.factors = growth_factors
        self.patch_size = patch_size
        self.model = None
        self.performChecks()
        self.last_eval_metrics = None  # will store last evaluation metrics here
        self.feature_map_history = []  # Store feature maps over time for analysis
        self.experiment_results = {}  # Store experiment results

    def performChecks(self):
        print("\nMatching the size of land cover and growth factors...")
        if (self.landcovers.row == self.factors.row) and (self.landcovers.col == self.factors.col):
            print("Size of rasters matched.")
            self.row = self.factors.row
            self.col = self.factors.col
        else:
            raise ValueError("ERROR! Raster size not matched please check.")

    def prepare_training_data(self):
        print("Preparing training data...")

        # We'll collect patches from transitions: 2015->2020 and optionally 2020->2025
        all_X = []
        all_y = []

        # Transition 2015 -> 2020
        input_stack_2015 = np.stack([self.landcovers.arr_lc1] +
                                    [self.factors.gf[i] for i in range(1, self.factors.nFactors + 1)], axis=0)
        # Create binary target: 1 for new urban expansion, 0 otherwise
        target_2020 = ((self.landcovers.arr_lc2 == 1) & (self.landcovers.arr_lc1 != 1)).astype(np.float32)
        target_2020 = np.expand_dims(target_2020, axis=0)
        input_stack_2015 = self.normalize_data(input_stack_2015)
        X_patches_1, y_patches_1 = self.create_patches(input_stack_2015, target_2020)

        if X_patches_1.size > 0:
            all_X.append(X_patches_1)
            all_y.append(y_patches_1)

        # Transition 2020 -> 2025 (if available)
        if self.landcovers.arr_lc3 is not None:
            input_stack_2020 = np.stack([self.landcovers.arr_lc2] +
                                        [self.factors.gf[i] for i in range(1, self.factors.nFactors + 1)], axis=0)
            target_2025 = ((self.landcovers.arr_lc3 == 1) & (self.landcovers.arr_lc2 != 1)).astype(np.float32)
            target_2025 = np.expand_dims(target_2025, axis=0)
            input_stack_2020 = self.normalize_data(input_stack_2020)
            X_patches_2, y_patches_2 = self.create_patches(input_stack_2020, target_2025)
            if X_patches_2.size > 0:
                all_X.append(X_patches_2)
                all_y.append(y_patches_2)

        if len(all_X) == 0:
            raise ValueError("No patches were created from any transition. Check inputs and patch size.")

        X_patches = np.concatenate(all_X, axis=0)
        y_patches = np.concatenate(all_y, axis=0)

        # Split into training and validation
        X_train, X_val, y_train, y_val = train_test_split(
            X_patches, y_patches, test_size=0.2, random_state=42
        )

        return X_train, X_val, y_train, y_val

    def normalize_data(self, data):
        """Normalize data to have zero mean and unit variance for each channel"""
        normalized_data = np.zeros_like(data, dtype=np.float32)
        for i in range(data.shape[0]):
            channel_data = data[i].astype(np.float32)
            mean = np.mean(channel_data)
            std = np.std(channel_data)
            if std > 0:
                normalized_data[i] = (channel_data - mean) / std
            else:
                normalized_data[i] = channel_data - mean
        return normalized_data

    def create_patches(self, input_data, target_data):
        patches = []
        target_patches = []
        # Number of non-overlapping patches
        num_patches_x = self.row // self.patch_size
        num_patches_y = self.col // self.patch_size

        for i in range(num_patches_x):
            for j in range(num_patches_y):
                patch = input_data[:,
                        i * self.patch_size:(i + 1) * self.patch_size,
                        j * self.patch_size:(j + 1) * self.patch_size
                        ]
                target_patch = target_data[:,
                               i * self.patch_size:(i + 1) * self.patch_size,
                               j * self.patch_size:(j + 1) * self.patch_size
                               ]
                # Keep patches with some change and sample some negatives
                if np.sum(target_patch) > 0 or np.random.random() > 0.7:
                    patches.append(patch)
                    target_patches.append(target_patch)

        if len(patches) == 0:
            return np.empty((0, input_data.shape[0], self.patch_size, self.patch_size), dtype=np.float32), \
                np.empty((0, 1, self.patch_size, self.patch_size), dtype=np.float32)

        return np.array(patches, dtype=np.float32), np.array(target_patches, dtype=np.float32)

    def create_patches_for_prediction(self, input_data):
        """Create patches for prediction without filtering"""
        patches = []
        positions = []
        num_patches_x = self.row // self.patch_size
        num_patches_y = self.col // self.patch_size

        for i in range(num_patches_x):
            for j in range(num_patches_y):
                patch = input_data[:,
                        i * self.patch_size:(i + 1) * self.patch_size,
                        j * self.patch_size:(j + 1) * self.patch_size
                        ]
                patches.append(patch)
                positions.append((i, j))

        return np.array(patches, dtype=np.float32), positions, num_patches_x, num_patches_y

    def build_model(self):
        input_channels = self.factors.nFactors + 1  # +1 for land cover channel
        self.model = ResConvLSTMModel(input_channels, self.patch_size).to(device)
        print(self.model)
        self.criterion = nn.BCELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5)

    def train(self, epochs=100, batch_size=16):
        X_train, X_val, y_train, y_val = self.prepare_training_data()
        print(f"Training samples: {len(X_train)}, Validation samples: {len(X_val)}")

        if len(X_train) == 0:
            raise ValueError("No training patches were generated. Check patch size or inputs.")

        train_dataset = UrbanExpansionDataset(X_train, y_train)
        val_dataset = UrbanExpansionDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
        best_val_loss = float('inf')

        for epoch in range(epochs):
            # Training phase
            self.model.train()
            train_loss = 0.0
            train_acc = 0.0
            train_batches = 0

            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                loss.backward()
                self.optimizer.step()

                train_loss += loss.item()
                preds = (outputs > 0.5).float()
                train_acc += (preds == batch_y).float().mean().item()
                train_batches += 1

            # Validation phase
            self.model.eval()
            val_loss = 0.0
            val_acc = 0.0
            val_batches = 0

            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    outputs = self.model(batch_X)
                    loss = self.criterion(outputs, batch_y)
                    val_loss += loss.item()
                    preds = (outputs > 0.5).float()
                    val_acc += (preds == batch_y).float().mean().item()
                    val_batches += 1

            train_loss = train_loss / train_batches if train_batches > 0 else 0.0
            train_acc = train_acc / train_batches if train_batches > 0 else 0.0
            val_loss = val_loss / val_batches if val_batches > 0 else 0.0
            val_acc = val_acc / val_batches if val_batches > 0 else 0.0

            self.scheduler.step(val_loss)

            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)

            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, '
                  f'Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

            # Store feature maps every 10 epochs for analysis
            if epoch % 10 == 0:
                with torch.no_grad():
                    sample_batch = next(iter(val_loader))[0][:1].to(device)
                    feature_maps = self.model.get_feature_maps(sample_batch)
                    self.feature_map_history.append({
                        'epoch': epoch,
                        'feature_maps': deepcopy(feature_maps)
                    })

        torch.save(self.model.state_dict(), 'final_model_no_attention.pth')

        return history

    def predict_next_year(self, current_landcover):
        """
        Predict urban expansion for the next year based on current landcover.
        FIXED: Properly preserves all land use categories.
        """
        # Store original landcover as integer type to preserve categories
        original_landcover = current_landcover.copy().astype(np.int32)

        # Create input stack for the model
        input_stack = np.stack([current_landcover] +
                               [self.factors.gf[i] for i in range(1, self.factors.nFactors + 1)], axis=0)
        input_stack = self.normalize_data(input_stack)
        patches, positions, num_patches_x, num_patches_y = self.create_patches_for_prediction(input_stack)

        if patches.size == 0:
            raise ValueError("No patches created for prediction. Check patch_size or raster sizes.")

        self.model.eval()
        predictions = []

        with torch.no_grad():
            for i in range(0, len(patches), 16):
                batch = torch.FloatTensor(patches[i:i + 16]).to(device)
                pred = self.model(batch)  # shape (batch, 1, patch, patch)
                predictions.append(pred.cpu().numpy())

        predictions = np.concatenate(predictions, axis=0)
        urban_expansion_prob = self.reconstruct_from_patches(predictions, positions, num_patches_x, num_patches_y)

        # Create constraints based on land use categories
        # 1 = urban (already urban, cannot expand here)
        # 2 = vegetation (can be converted to urban)
        # 3 = water (cannot be converted to urban)
        # 4 = paddy lands (can be converted to urban)

        # Create mask for areas that can potentially become urban
        # Only vegetation (2) and paddy lands (4) can be converted to urban
        convertible_mask = np.isin(original_landcover, [2, 4])

        # Apply probability threshold to determine which convertible areas become urban
        urban_expansion = np.zeros_like(urban_expansion_prob, dtype=np.int32)
        urban_expansion[convertible_mask] = (urban_expansion_prob[convertible_mask] > 0.5).astype(np.int32)

        # Create the next landcover map preserving original categories
        next_landcover = original_landcover.copy()

        # Only update cells that are predicted to become urban
        # This converts vegetation (2) or paddy lands (4) to urban (1)
        next_landcover[urban_expansion == 1] = 1

        # Ensure water (3) and already urban (1) areas remain unchanged
        # This is already guaranteed by our convertible_mask, but let's be explicit
        water_mask = original_landcover == 3
        next_landcover[water_mask] = 3  # Preserve water

        # Print statistics for debugging
        print(f"Urban cells before: {np.sum(original_landcover == 1)}")
        print(f"Urban cells after: {np.sum(next_landcover == 1)}")
        print(f"Vegetation cells before: {np.sum(original_landcover == 2)}")
        print(f"Vegetation cells after: {np.sum(next_landcover == 2)}")
        print(f"Water cells before: {np.sum(original_landcover == 3)}")
        print(f"Water cells after: {np.sum(next_landcover == 3)}")
        print(f"Paddy cells before: {np.sum(original_landcover == 4)}")
        print(f"Paddy cells after: {np.sum(next_landcover == 4)}")
        print(f"New urban conversions: {np.sum(urban_expansion == 1)}")

        return next_landcover.astype(np.int32), urban_expansion_prob

    def reconstruct_from_patches(self, patches, positions, num_patches_x, num_patches_y):
        reconstructed = np.zeros((self.row, self.col), dtype=np.float32)
        for idx, (i, j) in enumerate(positions):
            reconstructed[
            i * self.patch_size:(i + 1) * self.patch_size,
            j * self.patch_size:(j + 1) * self.patch_size
            ] = patches[idx, 0]
        return reconstructed

    def evaluate(self, actual_2025):
        """
        Evaluate predicted 2025 against actual_2025.
        Keeps original return signature (accuracy, f1, iou, predicted_2025)
        Also computes and prints:
         - Figure of Merit (FoM)
         - Allocation Disagreement (AD)
         - Quantity Disagreement (QD)
        Stores the full evaluation dictionary in self.last_eval_metrics
        """
        predicted_2025, _ = self.predict_next_year(self.landcovers.arr_lc2)

        # Ensure both are integer type for comparison
        actual_2025 = actual_2025.astype(np.int32)
        predicted_2025 = predicted_2025.astype(np.int32)

        actual_urban = (actual_2025 == 1).astype(np.int32)
        predicted_urban = (predicted_2025 == 1).astype(np.int32)
        eval_mask = (self.landcovers.arr_lc2 != 3)  # Exclude water from evaluation
        actual_flat = actual_urban[eval_mask].flatten()
        predicted_flat = predicted_urban[eval_mask].flatten()

        if actual_flat.size == 0:
            raise ValueError("No pixels available for evaluation (eval_mask empty).")

        accuracy = accuracy_score(actual_flat, predicted_flat)
        f1 = f1_score(actual_flat, predicted_flat, zero_division=0)
        iou = jaccard_score(actual_flat, predicted_flat, zero_division=0)

        # Additional land-change-specific metrics
        # Observed change area (A): pixels that are urban in actual_2025 but were not urban in 2020
        observed_change = ((actual_2025 == 1) & (self.landcovers.arr_lc2 != 1)).astype(np.int32)
        predicted_change = ((predicted_2025 == 1) & (self.landcovers.arr_lc2 != 1)).astype(np.int32)

        # Apply evaluation mask to change arrays
        observed_change_eval = observed_change[eval_mask].flatten()
        predicted_change_eval = predicted_change[eval_mask].flatten()

        # counts
        hits = int(np.sum((observed_change_eval == 1) & (predicted_change_eval == 1)))
        misses = int(np.sum((observed_change_eval == 1) & (predicted_change_eval == 0)))  # observed but not predicted
        false_alarms = int(
            np.sum((observed_change_eval == 0) & (predicted_change_eval == 1)))  # predicted but not observed
        total_eval_pixels = int(np.sum(eval_mask))

        # Figure of Merit (FoM): hits / (hits + misses + false_alarms)  (guard zero division)
        denom_fom = (hits + misses + false_alarms)
        if denom_fom > 0:
            fom = hits / denom_fom
        else:
            fom = 0.0

        # Quantity Disagreement (QD): |predicted_change_area - observed_change_area| / total_eval_pixels
        observed_change_area = int(np.sum(observed_change_eval))
        predicted_change_area = int(np.sum(predicted_change_eval))
        if total_eval_pixels > 0:
            qd = abs(predicted_change_area - observed_change_area) / total_eval_pixels
        else:
            qd = 0.0

        # Allocation Disagreement (AD): 2 * min(misses, false_alarms) / total_eval_pixels
        if total_eval_pixels > 0:
            ad = 2 * min(misses, false_alarms) / total_eval_pixels
        else:
            ad = 0.0

        # Print results
        print(f"Accuracy: {accuracy:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"IoU: {iou:.4f}")
        print(f"Figure of Merit (FoM): {fom:.4f}  (hits / (hits + misses + false alarms))")
        print(f"Allocation Disagreement (AD): {ad:.4f}  (2 * min(misses, false_alarms) / total_eval_pixels)")
        print(
            f"Quantity Disagreement (QD): {qd:.4f}  (|predicted_change_area - observed_change_area| / total_eval_pixels)")
        print(
            f"Hits: {hits}, Misses (observed-not-predicted): {misses}, False Alarms (predicted-not-observed): {false_alarms}")
        print(
            f"Observed change pixels: {observed_change_area}, Predicted change pixels: {predicted_change_area}, Total eval pixels: {total_eval_pixels}")

        # Store metrics for later inspection
        self.last_eval_metrics = {
            'accuracy': float(accuracy),
            'f1': float(f1),
            'iou': float(iou),
            'FoM': float(fom),
            'AllocationDisagreement': float(ad),
            'QuantityDisagreement': float(qd),
            'hits': hits,
            'misses': misses,
            'false_alarms': false_alarms,
            'observed_change_area': observed_change_area,
            'predicted_change_area': predicted_change_area,
            'total_eval_pixels': total_eval_pixels
        }

        return accuracy, f1, iou, predicted_2025

    def simulate_future(self, start_year, years=5):
        """
        Simulate future urban expansion while preserving land use categories.
        """
        current_landcover = start_year.copy().astype(np.int32)
        predictions = {}

        print(f"\nStarting simulation from base year with land use distribution:")
        print(f"  Urban (1): {np.sum(current_landcover == 1)} cells")
        print(f"  Vegetation (2): {np.sum(current_landcover == 2)} cells")
        print(f"  Water (3): {np.sum(current_landcover == 3)} cells")
        print(f"  Paddy lands (4): {np.sum(current_landcover == 4)} cells")

        for year in range(1, years + 1):
            print(f"\nSimulating year {year}...")
            current_landcover, _ = self.predict_next_year(current_landcover)
            # Ensure integer type is maintained
            current_landcover = current_landcover.astype(np.int32)
            predictions[year] = current_landcover.copy()

            # Verify land use categories are preserved
            unique_values = np.unique(current_landcover)
            print(f"  Unique land use values in year {year}: {unique_values}")
            print(f"  Land use distribution after year {year}:")
            print(f"    Urban (1): {np.sum(current_landcover == 1)} cells")
            print(f"    Vegetation (2): {np.sum(current_landcover == 2)} cells")
            print(f"    Water (3): {np.sum(current_landcover == 3)} cells")
            print(f"    Paddy lands (4): {np.sum(current_landcover == 4)} cells")

        return predictions

    ############################################################################
    # --- Advanced experiment methods (modified for feature map analysis) ---
    ############################################################################

    def run_advanced_experiments(self, X_val, y_val):
        """
        Run comprehensive experiments for research objectives
        """
        print("\n" + "=" * 80)
        print("RUNNING ADVANCED EXPERIMENTS FOR RESEARCH OBJECTIVES")
        print("=" * 80)

        # Store all experiment results
        self.experiment_results = {}

        # Experiment 1: Transition Rule Analysis (Variable Importance)
        print("\n>>> EXPERIMENT 1: VARIABLE IMPORTANCE ANALYSIS <<<")
        var_importance = self.analyze_variable_importance(X_val, y_val)
        self.experiment_results['variable_importance'] = var_importance

        # Experiment 2: Feature Map Analysis (replaces Attention Analysis)
        print("\n>>> EXPERIMENT 2: FEATURE MAP ANALYSIS <<<")
        feature_analysis = self.analyze_feature_maps()
        self.experiment_results['feature_analysis'] = feature_analysis

        # Experiment 3: Neighborhood Effect Analysis
        print("\n>>> EXPERIMENT 3: NEIGHBORHOOD EFFECT ANALYSIS <<<")
        neighborhood_analysis = self.analyze_neighborhood_effects(X_val)
        self.experiment_results['neighborhood_analysis'] = neighborhood_analysis

        # Experiment 4: Transition Pattern Analysis
        print("\n>>> EXPERIMENT 4: TRANSITION PATTERN ANALYSIS <<<")
        transition_patterns = self.analyze_transition_patterns(X_val, y_val)
        self.experiment_results['transition_patterns'] = transition_patterns

        # Experiment 5: Activation Analysis (new - similar to attention)
        print("\n>>> EXPERIMENT 5: ACTIVATION ANALYSIS <<<")
        activation_analysis = self.analyze_activations(X_val)
        self.experiment_results['activation_analysis'] = activation_analysis

        # Print comprehensive summary
        self.print_experiment_summary()

        return self.experiment_results

    def analyze_variable_importance(self, X_val, y_val):
        """
        Analyze importance of growth factors (excluding land cover)
        Using simple correlation and ablation analysis
        """
        print("  Analyzing variable importance...")

        # Ensure factor names match the order used when building input_stack
        feature_names = ['CBD', 'Road', 'Population', 'Slope', 'Restricted']
        importance_scores = {}

        # Method 1: Correlation analysis
        print("  Method 1: Correlation with predictions")
        with torch.no_grad():
            X_val_tensor = torch.FloatTensor(X_val).to(device)
            predictions = self.model(X_val_tensor).cpu().numpy().flatten()

        for i, factor_name in enumerate(feature_names):
            factor_idx = i + 1  # Skip land cover channel
            factor_values = X_val[:, factor_idx].flatten()

            # Compute correlation between factor values and predictions
            try:
                correlation = np.corrcoef(factor_values, predictions)[0, 1]
            except Exception:
                correlation = 0.0
            if np.isnan(correlation):
                correlation = 0.0
            importance_scores[factor_name] = float(correlation)
            print(f"    {factor_name}: correlation = {correlation:.4f}")

        # Method 2: Simple ablation (zero out each factor)
        print("  Method 2: Ablation analysis")
        baseline_accuracy = self._compute_baseline_accuracy(X_val, y_val)

        for i, factor_name in enumerate(feature_names):
            factor_idx = i + 1

            # Create ablated data (set factor to zero)
            X_ablated = X_val.copy()
            X_ablated[:, factor_idx] = 0  # Zero out the factor

            # Compute accuracy with ablated factor
            ablated_accuracy = self._compute_baseline_accuracy(X_ablated, y_val)
            importance_drop = baseline_accuracy - ablated_accuracy
            importance_scores[f"{factor_name}_ablation"] = float(importance_drop)
            print(f"    {factor_name}: accuracy drop = {importance_drop:.4f}")

        # Plot results
        plt.figure(figsize=(12, 5))

        # Correlation plot
        plt.subplot(1, 2, 1)
        corr_values = [importance_scores[name] for name in feature_names]
        plt.barh(feature_names, corr_values)
        plt.xlabel('Correlation with Predictions')
        plt.title('Variable Importance (Correlation)')

        # Ablation plot
        plt.subplot(1, 2, 2)
        ablation_values = [importance_scores[f"{name}_ablation"] for name in feature_names]
        plt.barh(feature_names, ablation_values)
        plt.xlabel('Accuracy Drop when Ablated')
        plt.title('Variable Importance (Ablation)')

        plt.tight_layout()
        plt.savefig('variable_importance_no_attention.png', dpi=300, bbox_inches='tight')
        plt.show()

        return importance_scores

    def _compute_baseline_accuracy(self, X, y):
        """Compute baseline accuracy"""
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(device)
            y_tensor = torch.FloatTensor(y).to(device)
            outputs = self.model(X_tensor)
            preds = (outputs > 0.5).float()
            accuracy = (preds == y_tensor).float().mean().item()
        return float(accuracy)

    def analyze_feature_maps(self):
        """
        Analyze feature maps from different layers (replaces attention analysis)
        """
        print("  Analyzing feature maps...")

        if not self.feature_map_history:
            print("    No feature map history available.")
            return {}

        latest_feature_maps = self.feature_map_history[-1]['feature_maps']
        feature_stats = {}

        print("  Latest Feature Map Statistics:")
        for layer_name, feature_data in latest_feature_maps.items():
            if feature_data is not None:
                flat_data = feature_data.flatten()
                stats = {
                    'mean': float(np.mean(flat_data)),
                    'std': float(np.std(flat_data)),
                    'min': float(np.min(flat_data)),
                    'max': float(np.max(flat_data)),
                    'variance': float(np.var(flat_data)),
                    'sparsity': float(np.mean(np.abs(flat_data) < 0.01))  # percentage of near-zero activations
                }
                feature_stats[layer_name] = stats

                print(f"    {layer_name}:")
                print(f"      Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
                print(f"      Range: [{stats['min']:.4f}, {stats['max']:.4f}]")
                print(f"      Sparsity: {stats['sparsity']:.4f}")

        # Visualize feature map distributions
        self._plot_feature_map_distributions(latest_feature_maps)

        # Analyze feature evolution
        if len(self.feature_map_history) > 1:
            evolution_stats = self._analyze_feature_evolution()
            feature_stats['evolution'] = evolution_stats

        return feature_stats

    def _plot_feature_map_distributions(self, feature_maps):
        """Plot distributions of feature map activations"""
        # Select key layers to visualize
        key_layers = ['resconvlstm1', 'resconvlstm4', 'resconvlstm7', 'resconvlstm8']

        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()

        for idx, layer_name in enumerate(key_layers):
            if layer_name in feature_maps and feature_maps[layer_name] is not None:
                flat_data = feature_maps[layer_name].flatten()
                # Sample for faster plotting if too many points
                if len(flat_data) > 10000:
                    flat_data = np.random.choice(flat_data, 10000, replace=False)
                axes[idx].hist(flat_data, bins=50, alpha=0.7, edgecolor='black')
                axes[idx].set_title(f'{layer_name} Distribution')
                axes[idx].set_xlabel('Activation Value')
                axes[idx].set_ylabel('Frequency')

        plt.tight_layout()
        plt.savefig('feature_map_distributions_no_attention.png', dpi=300, bbox_inches='tight')
        plt.show()

    def _analyze_feature_evolution(self):
        """Analyze how feature maps evolve during training"""
        print("  Analyzing feature evolution...")

        evolution_metrics = {}

        # Analyze key layers over time
        key_layers = ['resconvlstm1', 'resconvlstm4', 'resconvlstm7', 'resconvlstm8']

        for layer_name in key_layers:
            means_over_time = []
            stds_over_time = []
            epochs = []

            for record in self.feature_map_history:
                feature_data = record['feature_maps'].get(layer_name)
                if feature_data is not None:
                    means_over_time.append(np.mean(feature_data))
                    stds_over_time.append(np.std(feature_data))
                    epochs.append(record['epoch'])

            if means_over_time:
                # Compute stability metrics
                mean_stability = 1.0 / (np.var(means_over_time) + 1e-8)
                std_stability = 1.0 / (np.var(stds_over_time) + 1e-8)

                evolution_metrics[layer_name] = {
                    'mean_stability': float(mean_stability),
                    'std_stability': float(std_stability),
                    'final_mean': float(means_over_time[-1]),
                    'final_std': float(stds_over_time[-1])
                }

                print(f"    {layer_name}: mean stability = {mean_stability:.4f}, "
                      f"final mean = {means_over_time[-1]:.4f}")

        return evolution_metrics

    def analyze_activations(self, X_val, n_samples=10):
        """
        Analyze activation patterns across layers (similar to attention analysis)
        """
        print("  Analyzing activation patterns...")

        with torch.no_grad():
            sample_indices = np.random.choice(len(X_val), min(n_samples, len(X_val)), replace=False)
            X_sample = X_val[sample_indices]
            X_tensor = torch.FloatTensor(X_sample).to(device)

            # Get feature maps for analysis
            feature_maps = self.model.get_feature_maps(X_tensor)

        activation_analysis = {}

        for layer_name, activations in feature_maps.items():
            # Compute various activation statistics
            layer_activations = activations.flatten()

            stats = {
                'mean_activation': float(np.mean(layer_activations)),
                'std_activation': float(np.std(layer_activations)),
                'activation_sparsity': float(np.mean(np.abs(layer_activations) < 0.001)),
                'activation_entropy': self._compute_activation_entropy(layer_activations),
                'positive_ratio': float(np.mean(layer_activations > 0))
            }

            activation_analysis[layer_name] = stats

            print(f"    {layer_name}:")
            print(f"      Mean: {stats['mean_activation']:.4f}, Positive Ratio: {stats['positive_ratio']:.4f}")
            print(f"      Sparsity: {stats['activation_sparsity']:.4f}, Entropy: {stats['activation_entropy']:.4f}")

        # Visualize activation patterns
        self._plot_activation_patterns(activation_analysis)

        return activation_analysis

    def _compute_activation_entropy(self, activations):
        """Compute entropy of activation distribution"""
        # Discretize activations into bins for entropy calculation
        hist, _ = np.histogram(activations, bins=50, density=True)
        hist = hist[hist > 0]  # Remove zero bins
        hist = hist / np.sum(hist)  # Normalize to probability distribution
        entropy = -np.sum(hist * np.log(hist))
        return float(entropy)

    def _plot_activation_patterns(self, activation_analysis):
        """Plot activation patterns across layers"""
        layers = list(activation_analysis.keys())
        means = [activation_analysis[layer]['mean_activation'] for layer in layers]
        sparsities = [activation_analysis[layer]['activation_sparsity'] for layer in layers]
        positive_ratios = [activation_analysis[layer]['positive_ratio'] for layer in layers]

        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # Mean activations
        axes[0].barh(layers, means)
        axes[0].set_xlabel('Mean Activation')
        axes[0].set_title('Mean Activations by Layer')

        # Sparsity
        axes[1].barh(layers, sparsities)
        axes[1].set_xlabel('Sparsity (fraction < 0.001)')
        axes[1].set_title('Activation Sparsity by Layer')

        # Positive ratio
        axes[2].barh(layers, positive_ratios)
        axes[2].set_xlabel('Positive Activation Ratio')
        axes[2].set_title('Positive Activation Ratio by Layer')

        plt.tight_layout()
        plt.savefig('activation_patterns_no_attention.png', dpi=300, bbox_inches='tight')
        plt.show()

    def analyze_neighborhood_effects(self, X_val):
        """
        Analyze neighborhood effects using spatial analysis
        """
        print("  Analyzing neighborhood effects...")

        neighborhood_results = {}

        # Method 1: Spatial autocorrelation analysis
        spatial_autocorr = self._compute_spatial_autocorrelation_batch(X_val)
        neighborhood_results['spatial_autocorrelation'] = float(spatial_autocorr)
        print(f"    Spatial autocorrelation: {spatial_autocorr:.4f}")

        # Method 2: Urban cluster analysis
        cluster_stats = self._analyze_urban_clusters(X_val)
        neighborhood_results['cluster_analysis'] = cluster_stats
        print(f"    Average urban cluster size: {cluster_stats['avg_cluster_size']:.2f}")
        print(f"    Cluster density: {cluster_stats['cluster_density']:.4f}")

        # Method 3: Edge effect analysis
        edge_effects = self._analyze_edge_effects(X_val)
        neighborhood_results['edge_effects'] = edge_effects
        print(f"    Edge-to-interior ratio: {edge_effects['edge_ratio']:.4f}")

        # Plot neighborhood analysis
        self._plot_neighborhood_analysis(neighborhood_results)

        return neighborhood_results

    def _compute_spatial_autocorrelation_batch(self, X_val, n_samples=20):
        """Compute spatial autocorrelation for a batch of samples"""
        with torch.no_grad():
            # Use a subset for efficiency
            sample_indices = np.random.choice(len(X_val), min(n_samples, len(X_val)), replace=False)
            X_sample = X_val[sample_indices]
            X_tensor = torch.FloatTensor(X_sample).to(device)
            predictions = self.model(X_tensor).cpu().numpy()

        autocorrelations = []
        for i in range(len(predictions)):
            patch = predictions[i, 0]
            autocorr = self._compute_morans_i_simple(patch)
            if not np.isnan(autocorr):
                autocorrelations.append(autocorr)

        return float(np.mean(autocorrelations)) if autocorrelations else 0.0

    def _compute_morans_i_simple(self, data):
        """Simple Moran's I computation"""
        n = data.shape[0]
        mean_val = np.mean(data)

        numerator = 0.0
        denominator = 0.0

        for i in range(n):
            for j in range(n):
                if i > 0:  # Left neighbor
                    numerator += (data[i, j] - mean_val) * (data[i - 1, j] - mean_val)
                if i < n - 1:  # Right neighbor
                    numerator += (data[i, j] - mean_val) * (data[i + 1, j] - mean_val)
                if j > 0:  # Top neighbor
                    numerator += (data[i, j] - mean_val) * (data[i, j - 1] - mean_val)
                if j < n - 1:  # Bottom neighbor
                    numerator += (data[i, j] - mean_val) * (data[i, j + 1] - mean_val)

                denominator += (data[i, j] - mean_val) ** 2

        if denominator == 0:
            return 0.0

        W = 4 * n * n - 2 * n - 2 * n  # Total weights (approx)
        morans_i = (n * numerator) / (W * denominator)
        return float(morans_i)

    def _analyze_urban_clusters(self, X_val, n_samples=10):
        """Analyze urban cluster patterns in predictions"""
        with torch.no_grad():
            sample_indices = np.random.choice(len(X_val), min(n_samples, len(X_val)), replace=False)
            X_sample = X_val[sample_indices]
            X_tensor = torch.FloatTensor(X_sample).to(device)
            predictions = self.model(X_tensor).cpu().numpy()

        cluster_sizes = []
        urban_densities = []

        for i in range(len(predictions)):
            binary_urban = (predictions[i, 0] > 0.5).astype(int)

            # Label connected components
            from scipy import ndimage
            labeled_array, num_features = ndimage.label(binary_urban)

            if num_features > 0:
                cluster_sizes.extend([int(np.sum(labeled_array == j)) for j in range(1, num_features + 1)])
            urban_densities.append(float(np.mean(binary_urban)))

        return {
            'avg_cluster_size': float(np.mean(cluster_sizes)) if cluster_sizes else 0.0,
            'max_cluster_size': int(np.max(cluster_sizes)) if cluster_sizes else 0,
            'cluster_density': float(np.mean(urban_densities)) if urban_densities else 0.0,
            'n_clusters_analyzed': len(cluster_sizes)
        }

    def _analyze_edge_effects(self, X_val, n_samples=10):
        """Analyze edge effects in urban predictions"""
        with torch.no_grad():
            sample_indices = np.random.choice(len(X_val), min(n_samples, len(X_val)), replace=False)
            X_sample = X_val[sample_indices]
            X_tensor = torch.FloatTensor(X_sample).to(device)
            predictions = self.model(X_tensor).cpu().numpy()

        edge_ratios = []

        for i in range(len(predictions)):
            binary_urban = (predictions[i, 0] > 0.5).astype(int)

            # Define edge (2-pixel border) and interior
            border_width = 2
            edge_mask = np.zeros_like(binary_urban, dtype=bool)
            edge_mask[:border_width, :] = True  # Top
            edge_mask[-border_width:, :] = True  # Bottom
            edge_mask[:, :border_width] = True  # Left
            edge_mask[:, -border_width:] = True  # Right

            interior_mask = ~edge_mask

            edge_urban_density = float(np.mean(binary_urban[edge_mask]))
            interior_urban_density = float(np.mean(binary_urban[interior_mask]))

            if interior_urban_density > 0:
                edge_ratio = edge_urban_density / interior_urban_density
            else:
                edge_ratio = edge_urban_density

            edge_ratios.append(edge_ratio)

        return {
            'edge_ratio': float(np.mean(edge_ratios)) if edge_ratios else 0.0,
            'edge_density': float(np.mean([r for r in edge_ratios if not np.isnan(r)])) if edge_ratios else 0.0
        }

    def _plot_neighborhood_analysis(self, results):
        """Plot neighborhood analysis results"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # Spatial autocorrelation
        axes[0].bar(['Spatial Autocorrelation'], [results['spatial_autocorrelation']])
        axes[0].set_ylabel("Moran's I")
        axes[0].set_title('Spatial Autocorrelation')

        # Cluster analysis
        cluster_data = results['cluster_analysis']
        axes[1].bar(['Avg Cluster Size', 'Cluster Density'],
                    [cluster_data['avg_cluster_size'], cluster_data['cluster_density']])
        axes[1].set_title('Urban Cluster Analysis')

        # Edge effects
        edge_data = results['edge_effects']
        axes[2].bar(['Edge/Interior Ratio'], [edge_data['edge_ratio']])
        axes[2].set_title('Edge Effects')

        plt.tight_layout()
        plt.savefig('neighborhood_analysis_no_attention.png', dpi=300, bbox_inches='tight')
        plt.show()

    def analyze_transition_patterns(self, X_val, y_val):
        """
        Analyze transition patterns and rule consistency
        """
        print("  Analyzing transition patterns...")

        pattern_results = {}

        # Analyze transition probabilities by land cover type
        transition_probs = self._analyze_transition_by_landcover(X_val)
        pattern_results['transition_probabilities'] = transition_probs

        # Analyze spatial consistency
        spatial_consistency = self._analyze_spatial_consistency(X_val)
        pattern_results['spatial_consistency'] = spatial_consistency
        print(f"    Spatial consistency: {spatial_consistency:.4f}")

        # Analyze prediction confidence
        confidence_stats = self._analyze_prediction_confidence(X_val)
        pattern_results['confidence_stats'] = confidence_stats
        print(f"    Average prediction confidence: {confidence_stats['mean_confidence']:.4f}")

        return pattern_results

    def _analyze_transition_by_landcover(self, X_val):
        """Analyze transition probabilities by original land cover type"""
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_val).to(device)
            predictions = self.model(X_tensor).cpu().numpy().flatten()

        land_cover_vals = X_val[:, 0].flatten()

        transition_probs = {}
        for land_cover in [2, 4]:  # Vegetation and Paddy
            mask = (land_cover_vals == land_cover)
            if np.sum(mask) > 0:
                mean_prob = float(np.mean(predictions[mask]))
                std_prob = float(np.std(predictions[mask]))
                transition_probs[land_cover] = {
                    'mean': mean_prob,
                    'std': std_prob,
                    'n_samples': int(np.sum(mask))
                }
                print(f"    Land cover {land_cover}: {mean_prob:.4f}  {std_prob:.4f}")

        return transition_probs

    def _analyze_spatial_consistency(self, X_val, n_samples=20):
        """Analyze spatial consistency of predictions"""
        with torch.no_grad():
            sample_indices = np.random.choice(len(X_val), min(n_samples, len(X_val)), replace=False)
            X_sample = X_val[sample_indices]
            X_tensor = torch.FloatTensor(X_sample).to(device)
            predictions = self.model(X_tensor).cpu().numpy()

        consistencies = []
        for i in range(len(predictions)):
            patch = predictions[i, 0]
            # Compute local variance (inverse of consistency)
            from scipy.ndimage import uniform_filter
            local_mean = uniform_filter(patch, size=3)
            local_var = uniform_filter(patch ** 2, size=3) - local_mean ** 2
            consistency = 1.0 / (np.mean(local_var) + 1e-8)
            consistencies.append(consistency)

        return float(np.mean(consistencies)) if consistencies else 0.0

    def _analyze_prediction_confidence(self, X_val):
        """Analyze prediction confidence distribution"""
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_val).to(device)
            predictions = self.model(X_tensor).cpu().numpy().flatten()

        # Measure distance from decision boundary (0.5)
        confidence = 2 * np.abs(predictions - 0.5)

        return {
            'mean_confidence': float(np.mean(confidence)),
            'std_confidence': float(np.std(confidence)),
            'high_confidence_ratio': float(np.mean(confidence > 0.8))  # >80% confidence
        }

    def print_experiment_summary(self):
        """Print comprehensive summary of all experiments"""
        print("\n" + "=" * 80)
        print("COMPREHENSIVE EXPERIMENT SUMMARY")
        print("=" * 80)

        # Original evaluation metrics
        if self.last_eval_metrics:
            print("\nORIGINAL EVALUATION METRICS:")
            print(f"  Accuracy: {self.last_eval_metrics['accuracy']:.4f}")
            print(f"  F1 Score: {self.last_eval_metrics['f1']:.4f}")
            print(f"  IoU: {self.last_eval_metrics['iou']:.4f}")
            print(f"  Figure of Merit (FoM): {self.last_eval_metrics['FoM']:.4f}")
            print(f"  Allocation Disagreement: {self.last_eval_metrics['AllocationDisagreement']:.4f}")
            print(f"  Quantity Disagreement: {self.last_eval_metrics['QuantityDisagreement']:.4f}")

        # Variable Importance Summary
        if 'variable_importance' in self.experiment_results:
            print("\nVARIABLE IMPORTANCE SUMMARY:")
            var_imp = self.experiment_results['variable_importance']
            # Get correlation scores for main factors
            factors = ['CBD', 'Road', 'Population', 'Slope', 'Restricted']
            for factor in factors:
                if factor in var_imp:
                    print(f"  {factor}: {var_imp[factor]:.4f}")

        # Feature Map Analysis Summary
        if 'feature_analysis' in self.experiment_results:
            print("\nFEATURE MAP ANALYSIS SUMMARY:")
            feature_analysis = self.experiment_results['feature_analysis']
            key_layers = ['resconvlstm1', 'resconvlstm4', 'resconvlstm7', 'resconvlstm8']
            for layer in key_layers:
                if layer in feature_analysis:
                    stats = feature_analysis[layer]
                    print(f"  {layer}: mean={stats['mean']:.4f}, sparsity={stats['sparsity']:.4f}")

        # Activation Analysis Summary
        if 'activation_analysis' in self.experiment_results:
            print("\nACTIVATION ANALYSIS SUMMARY:")
            activation_analysis = self.experiment_results['activation_analysis']
            key_layers = ['resconvlstm1', 'resconvlstm4', 'resconvlstm7', 'resconvlstm8']
            for layer in key_layers:
                if layer in activation_analysis:
                    stats = activation_analysis[layer]
                    print(
                        f"  {layer}: mean={stats['mean_activation']:.4f}, positive_ratio={stats['positive_ratio']:.4f}")

        # Neighborhood Effects Summary
        if 'neighborhood_analysis' in self.experiment_results:
            print("\nNEIGHBORHOOD EFFECTS SUMMARY:")
            neigh_analysis = self.experiment_results['neighborhood_analysis']
            print(f"  Spatial Autocorrelation: {neigh_analysis['spatial_autocorrelation']:.4f}")
            print(f"  Avg Urban Cluster Size: {neigh_analysis['cluster_analysis']['avg_cluster_size']:.2f}")
            print(f"  Edge/Interior Ratio: {neigh_analysis['edge_effects']['edge_ratio']:.4f}")

        # Transition Patterns Summary
        if 'transition_patterns' in self.experiment_results:
            print("\nTRANSITION PATTERNS SUMMARY:")
            patterns = self.experiment_results['transition_patterns']
            print(f"  Spatial Consistency: {patterns['spatial_consistency']:.4f}")
            print(f"  Avg Prediction Confidence: {patterns['confidence_stats']['mean_confidence']:.4f}")

        print("\n" + "=" * 80)


# Save predicted map function
def exportPredicted(array, outFileName, template_ds):
    """
    Export predicted land cover map as GeoTIFF.
    FIXED: Ensures integer data type for categorical land use.
    """
    driver = gdal.GetDriverByName("GTiff")
    # Use GDT_Int16 for categorical data instead of GDT_Float32
    outdata = driver.Create(outFileName, template_ds.RasterXSize, template_ds.RasterYSize, 1, gdal.GDT_Int16)
    outdata.SetGeoTransform(template_ds.GetGeoTransform())
    outdata.SetProjection(template_ds.GetProjection())
    # Ensure array is integer type before writing
    array_int = array.astype(np.int16)
    outdata.GetRasterBand(1).WriteArray(array_int)
    outdata.GetRasterBand(1).SetNoDataValue(0)
    outdata.FlushCache()
    outdata = None
    print(f"Exported {outFileName} with land use categories: {np.unique(array_int)}")


##################################################
## Main execution
##################################################

if __name__ == "__main__":
    # Input land cover GeoTIFF for three time periods
    file_2015 = "DataColombo/2015_cleaned.tif"
    file_2020 = "DataColombo/2020_cleaned.tif"
    file_2025 = "DataColombo/2025_cleaned.tif"  # For validation and optional training

    # Input all the spatial variables (check these file names exist in your folder)
    cbd = "DataColombo/CBD_cleaned (1).tif"
    road = "DataColombo/road_cleaned.tif"
    restricted = "DataColombo/restricted_cleaned.tif"
    pop24 = "DataColombo/pop_cleaned.tif"
    slope = "DataColombo/slope_cleaned.tif"

    # Create land cover and factors
    print("=== Loading Land Cover Data ===")
    myLandcover = LandCoverData(file_2015, file_2020, file_2025)

    # Print initial land use distribution
    print("\n=== Initial Land Use Distribution ===")
    print("2015 Land Use:")
    print(f"  Urban (1): {np.sum(myLandcover.arr_lc1 == 1)} cells")
    print(f"  Vegetation (2): {np.sum(myLandcover.arr_lc1 == 2)} cells")
    print(f"  Water (3): {np.sum(myLandcover.arr_lc1 == 3)} cells")
    print(f"  Paddy lands (4): {np.sum(myLandcover.arr_lc1 == 4)} cells")

    print("\n2020 Land Use:")
    print(f"  Urban (1): {np.sum(myLandcover.arr_lc2 == 1)} cells")
    print(f"  Vegetation (2): {np.sum(myLandcover.arr_lc2 == 2)} cells")
    print(f"  Water (3): {np.sum(myLandcover.arr_lc2 == 3)} cells")
    print(f"  Paddy lands (4): {np.sum(myLandcover.arr_lc2 == 4)} cells")

    if myLandcover.arr_lc3 is not None:
        print("\n2025 Land Use (Actual):")
        print(f"  Urban (1): {np.sum(myLandcover.arr_lc3 == 1)} cells")
        print(f"  Vegetation (2): {np.sum(myLandcover.arr_lc3 == 2)} cells")
        print(f"  Water (3): {np.sum(myLandcover.arr_lc3 == 3)} cells")
        print(f"  Paddy lands (4): {np.sum(myLandcover.arr_lc3 == 4)} cells")

    print("\n=== Loading Growth Factors ===")
    myFactors = GrowthFactors(cbd, road, pop24, slope, restricted)

    # Initialize model
    print("\n=== Initializing Deep Learning CA Model (No Attention) ===")
    dl_ca_model = DeepLearningCA(myLandcover, myFactors, patch_size=64)
    dl_ca_model.build_model()

    # Prepare data for experiments
    print("\n=== Preparing Data for Advanced Experiments ===")
    X_train, X_val, y_train, y_val = dl_ca_model.prepare_training_data()

    # Train
    print("\n=== Training Model ===")
    start_time = time.time()
    history = dl_ca_model.train(epochs=100, batch_size=16)
    end_time = time.time()
    print(f"Training time: {(end_time - start_time) / 60:.2f} minutes")

    # Evaluate on 2025 (if available) - This will compute original metrics
    predicted_2025 = None
    if myLandcover.arr_lc3 is not None:
        print("\n=== Evaluating Model on 2025 ===")
        accuracy, f1, iou, predicted_2025 = dl_ca_model.evaluate(myLandcover.arr_lc3)
        exportPredicted(predicted_2025, 'predicted_2025_no_attention.tif', myLandcover.ds_lc1)
        print("Saved 2025 prediction for accuracy assessment")

        print("\n2025 Land Use (Predicted):")
        print(f"  Urban (1): {np.sum(predicted_2025 == 1)} cells")
        print(f"  Vegetation (2): {np.sum(predicted_2025 == 2)} cells")
        print(f"  Water (3): {np.sum(predicted_2025 == 3)} cells")
        print(f"  Paddy lands (4): {np.sum(predicted_2025 == 4)} cells")

    # Run advanced experiments
    experiment_results = dl_ca_model.run_advanced_experiments(X_val, y_val)

    # Simulate future expansion
    print("\n=== Simulating Future Urban Expansion ===")
    if myLandcover.arr_lc3 is not None:
        start_year = myLandcover.arr_lc3
        print("Using actual 2025 as base for future simulation")
    elif predicted_2025 is not None:
        start_year = predicted_2025
        print("Using predicted 2025 as base for future simulation")
    else:
        # fallback to 2020 if nothing else available
        start_year = myLandcover.arr_lc2
        print("Using 2020 as base for future simulation")

    # Simulate to 2030
    print("\n=== Simulating 2025-2030 ===")
    future_predictions = dl_ca_model.simulate_future(start_year, years=5)
    if 5 in future_predictions:
        exportPredicted(future_predictions[5], 'predicted_2030_no_attention.tif', myLandcover.ds_lc1)
        print("\nSaved 2030 prediction")

        print("\n2030 Land Use (Predicted):")
        print(f"  Urban (1): {np.sum(future_predictions[5] == 1)} cells")
        print(f"  Vegetation (2): {np.sum(future_predictions[5] == 2)} cells")
        print(f"  Water (3): {np.sum(future_predictions[5] == 3)} cells")
        print(f"  Paddy lands (4): {np.sum(future_predictions[5] == 4)} cells")

    # Continue to 2035
    print("\n=== Simulating 2030-2035 ===")
    future_predictions_2035 = dl_ca_model.simulate_future(future_predictions[5], years=5)
    exportPredicted(future_predictions_2035[5], 'predicted_2035_no_attention.tif', myLandcover.ds_lc1)
    print("\nSaved 2035 prediction")

    print("\n2035 Land Use (Predicted):")
    print(f"  Urban (1): {np.sum(future_predictions_2035[5] == 1)} cells")
    print(f"  Vegetation (2): {np.sum(future_predictions_2035[5] == 2)} cells")
    print(f"  Water (3): {np.sum(future_predictions_2035[5] == 3)} cells")
    print(f"  Paddy lands (4): {np.sum(future_predictions_2035[5] == 4)} cells")

    # Plot training history
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Model Loss (No Attention)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Training Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.title('Model Accuracy (No Attention)')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.tight_layout()
    plt.savefig('training_history_no_attention.png')
    plt.show()

    print("\n=== Model training and experiments completed successfully! ===")
    print("\nAll experimental results, visualizations, and predictions have been saved to disk.")

